"""想要使用python做文本分析，分词是必不可少的一个环节
jieba模块使用总结
1、jieba（结巴）是一个强大的分词库，完美支持中文分词
（中文分词： 指的是将一个汉字序列切分成一个个单独的词。
分词就是将连续的字序列按照一定的规范重新组合成词序列的过程。）
我们知道，在英文的行文中，单词之间是以空格作为自然分界符的，而中文只是
字、句和段能通过明显的分界符来简单划界，唯独词没有一个形式上的分界符，
虽然英文也同样存在短语的划分问题，不过在词这一层上，中文比之英文要复杂得多、困难
得多。
2、安装jieba  pip install jieba
3、结巴分词分为三种模式：精确模式（默认）、全模式和搜索引擎模式
精确模式：试图将语句最精确的切分，不存在冗余数据，适合做文本分析

全模式：将语句中所有可能是词的词语都切分出来，速度很快，但是存在冗余数据

搜索引擎模式：在精确模式的基础上，对长词再次进行切分
cut(self, sentence, cut_all=False, HMM=True)
三个参数意思为： 输入的文本，是否为全模式分词 与是否开启HMM进行中文分词

"""
import jieba
#默认模式（精确模式）
s = '我想和女朋友一起去北京故宫博物院参观和闲逛。'
cut=jieba.cut(s)
#分词结果返回的是一个生成器（这对大数据量数据的分词尤为重要）
print(cut,type(cut))#结果<class 'generator'>
print(",".join(cut))#我,想,和,女朋友,一起,去,北京故宫博物院,参观,和,闲逛,。

#全模式就是把文本分成尽可能多的词。
cut=jieba.cut(s,cut_all=True)
print(",".join(cut))
#我,想,和,女朋友,朋友,一起,去,北京,北京故宫,北京故宫博物院,故宫,故宫博物院,博物,博物院,参观,和,闲逛,

#搜索引擎模式
slist=jieba.cut_for_search(s)
#我,想,和,朋友,女朋友,一起,去,北京,故宫,博物,博物院,北京故宫博物院,参观,和,闲逛,。
print(",".join(slist))
#-------------------------------------------------------------------------------
"""每个词都有其词性，比如名词、动词、代词等，
结巴分词的结果也可以带上每个词的词性，要用到jieba.posseg"""
#获取词性---------------------------------
s='我想和女朋友一起去北京故宫博物院参观和闲逛。'
print([(x.word,x.flag) for x in psg.cut(s)])
"""[('我', 'r'), ('想', 'v'), ('和', 'c'), ('女朋友', 'n'), 
('一起', 'm'), ('去', 'v'), ('北京故宫博物院', 'ns'),
 ('参观', 'n'), ('和', 'c'), ('闲逛', 'v'), ('。', 'x')]"""
#想了解更详细的词性分类信息，可以到网上搜索"结巴分词词性对照"。
print([(x.word,x.flag) for x in psg.cut(s) if x.flag.startswith("n")])
#[('女朋友', 'n'), ('北京故宫博物院', 'ns'), ('参观', 'n')]

"""
•开发者可以指定自己自定义的词典，以便包含 jieba 词库里没有的词。虽然 jieba 
有新词识别能力，但是自行添加新词可以保证更高的正确率
•用法： jieba.load_userdict(file_name) # file_name 为文件类对象或自定义词典的路径
•词典格式和 dict.txt 一样，一个词占一行；每一行分三部分：词语、词频（可省略）、
词性（可省略），用空格隔开，顺序不可颠倒。file_name 若为路径或二进制方式打开的文件，
则文件必须为 UTF-8 编码。
•词频省略时使用自动计算的能保证分出该词的词频。

"""
import jieba
s="李小福是创新办主任也是云计算方面的专家"
temp=jieba.cut(s)
print(",".join(temp))
#自定义词典（创新办 5 nr 云计算 10 nr）
jieba.load_userdict(r"D:\Users\Administrator\PycharmProjects\test\dict.txt")
temp=jieba.cut(s)
print(",".join(temp))












